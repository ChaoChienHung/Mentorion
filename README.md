# AI Research Assistant â€“ Development Notes

æœ¬æ–‡ä»¶ç´€éŒ„ **AI Research Assistant** å°ˆæ¡ˆçš„è¨­è¨ˆæ€è·¯ã€ç¨‹å¼æ¶æ§‹ã€å¾…å®ŒæˆåŠŸèƒ½èˆ‡ Mock å¯¦ä½œç´°ç¯€ã€‚  
ç›®æ¨™ï¼šä»¥ **çˆ¬èŸ² â†’ çµæ§‹åŒ–æŠ½å– â†’ï¼ˆå¯é¸ï¼‰LLM Function Calling â†’ å›ºå®šé‚è¼¯åˆ†æ** çš„æ–¹å¼ï¼Œå®Œæˆå°å‹ç ”ç©¶ç³»çµ±ã€‚

---

## ğŸ—ï¸ æ•´é«”æ¶æ§‹èˆ‡æµç¨‹

```
flowchart TD
A[URL] --> Wikipedia Parser --> B[Raw Text]
B[Raw Text] --> Wikipedia Parser clean_content (HTML Parser) --> C[JSON Format]
C --> D[Agent Class]
D --> D1[Structured Extraction (Pydantic Schema)]
D --> D2[Store Extracted Articles]
D --> D3[Function Calling Layer]
D3 -->|compare_technologies() or trace_evolution()| E[External Tool / Gemini API]
D3 -->|Mock Mode| F[Basic JSON/Dict Extraction + Print Summary]
```
- **Raw Text Cleaning**ï¼šå° HTML/ç´”æ–‡å­—é€²è¡Œåˆæ­¥æ¸…ç†ï¼ˆç§»é™¤æ¨™ç±¤ã€å¤šé¤˜ç©ºæ ¼ç­‰ï¼‰ã€‚
- **HTML Parser â†’ JSON Format**ï¼šè§£æ HTMLï¼Œå°‡çµæ§‹åŒ–è³‡è¨Šï¼ˆæ¨™é¡Œã€Headerã€æ®µè½ç­‰ï¼‰è½‰æˆ JSON/dictã€‚
- **Agent** Classï¼šæ ¸å¿ƒç®¡ç†æ¨¡çµ„ï¼Œå…§å«ï¼š
    - **Structured Extraction** (Pydantic Schema)ï¼šåˆ©ç”¨ Pydantic Schema é©—è­‰èˆ‡å­˜æ”¾æ–‡ç« è³‡æ–™ã€‚
    - **Store Extracted Articles**ï¼šé›†ä¸­å„²å­˜æ‰€æœ‰å·²è™•ç†æ–‡ç« ï¼Œä¾¿æ–¼å¾ŒçºŒæ“ä½œã€‚
    - **Function Calling Layer**ï¼šæ ¹æ“š LLM è¼¸å…¥è‡ªå‹•æ±ºå®šï¼š
        - **å‘¼å«å¤–éƒ¨å·¥å…·**ï¼ˆå¦‚ Gemini APIï¼‰
        - **æˆ–é€²å…¥ Mock æ¨¡å¼**ï¼ˆä»¥ JSON/dict è¼¸å‡ºä¸¦åšç°¡å–®æ‘˜è¦ï¼‰ã€‚

---

## ğŸ“– Wikipedia WebScraper

é€™æ˜¯ä¸€å€‹åŸºæ–¼ **Python + Crawl4AI + OpenAI API** çš„ **éåŒæ­¥ Wikipedia çˆ¬èŸ²**ï¼Œå¯ä»¥æ‰¹é‡æŠ“å–æ–‡ç« å…§å®¹ï¼Œä¸¦å°‡å…§å®¹è½‰æ›æˆä¹¾æ·¨çš„æ–‡å­—ï¼Œä»¥åˆ©å¾ŒçºŒé€²è¡Œ NLP æˆ– LLM çŸ¥è­˜åº«å»ºæ§‹ã€‚

---

### ğŸš€ åŠŸèƒ½ç‰¹è‰²

- æ”¯æ´ **å¤šç¶²å€éåŒæ­¥çˆ¬å–**ï¼ˆ`asyncio`ï¼‰
- å›å‚³çµæ§‹åŒ–è³‡è¨Šï¼ˆæ¨™é¡Œã€å­—æ•¸ã€é€£çµæ•¸é‡ã€è™•ç†æ™‚é–“ï¼‰
- å…§å»º **Markdown â†’ ç´”æ–‡å­—** æ¸…ç†åŠŸèƒ½
- æ”¯æ´ **å®‰å…¨ API Key è¼¸å…¥** èˆ‡ **OpenAI client é©—è­‰**
- å¯æ“´å……è‡³ **è‡ªå‹• chunking / LLM-based extraction**
- **é€Ÿç‡é™åˆ¶ï¼ˆRate Limitingï¼‰**ï¼šé¿å…éåº¦è«‹æ±‚

---

## âœ… å¾…å®Œæˆæ¸…å–®ï¼ˆChecklistï¼‰

- [ ] WikipediaExtraction Error Case Handling
- [ ] æ¸¬è©¦ï¼šç©ºè³‡æ–™ã€æ‰¾ä¸åˆ°æ¨™é¡Œã€category ç‚ºç©ºã€JSON æ ¼å¼éŒ¯èª¤ç­‰ã€‚  
- [ ] æ–‡ä»¶åŒ–ï¼šåœ¨ README ä¸­åŠ å…¥ä½¿ç”¨ç¤ºä¾‹èˆ‡ CLI/Notebook ç¯„ä¾‹ã€‚

### ğŸ“¦ å®‰è£éœ€æ±‚

#### Dependencies
* crawl4ai>=0.2.0
* openai>=1.0.0
* pydantic>=2.0.0
* python-dotenv>=1.0.0
* requests>=2.25.0
* beautifulsoup4>=4.9.0

**è«‹å…ˆå®‰è£å¿…è¦å¥—ä»¶ï¼š**
  
```bash
pip install crawl4ai ratelimit beautifulsoup4 markdown openai pandas requests
```

#### ğŸ›  ä½¿ç”¨æ–¹å¼
1. API Key è¨­å®š
è«‹åœ¨åŸ·è¡Œç¨‹å¼æ™‚è¼¸å…¥ API Keyï¼Œæˆ–æ˜¯é å…ˆåœ¨ç’°å¢ƒè®Šæ•¸ä¸­è¨­å®šï¼š
`export OPENAI_API_KEY="your_key_here"`

2. å»ºç«‹ Scraper ä¸¦åŸ·è¡Œ
```python
import asyncio
from scraper import WikipediaScraper  # å‡è¨­ä½ æŠŠ class å­˜åœ¨ scraper.py

urls = [
    "https://en.wikipedia.org/wiki/Natural_language_processing",
    "https://en.wikipedia.org/wiki/Machine_learning"
]

scraper = WikipediaScraper(base_urls=urls)

results = asyncio.run(scraper.scrape_multiple())

for r in results:
    print(r["title"], r["markdown_length"], "chars")
```

#### ğŸ“‚ å›å‚³çµæœæ ¼å¼

`scrape_article` å›å‚³çš„çµæœæœƒæ˜¯ JSON-like dictï¼š
```bash
{
  "title": "Natural language processing",
  "html": "...",
  "markdown": "...",
  "html_length": 15234,
  "markdown_length": 8921,
  "links_found": 245,
  "crawl_time": 1735712456.2381
}
```

#### ğŸ§¹ æ¸…ç†å…§å®¹
ä½ å¯ä»¥ä½¿ç”¨ clean_content() å°‡ Markdown è½‰æ›æˆä¹¾æ·¨çš„ç´”æ–‡å­—ï¼š
```python
raw_markdown = results[0]["markdown"]
clean_text = scraper.clean_content(raw_markdown)

print(clean_text[:500])  # é¡¯ç¤ºå‰ 500 å­—
```

`clean_content` å›å‚³çš„çµæœæœƒæ˜¯ JSON-like stringï¼š
```bash
{
    "title": "Natural language processing",
    "content" {
        "History": "...",
        "See also": "...",
    }
}
```
#### ğŸ”’ OpenAI Client å»ºç«‹èˆ‡æ¸¬è©¦

æ­¤å°ˆæ¡ˆå…§å»º `create_secure_openai_client()`ï¼Œæœƒï¼š
- å¾ç’°å¢ƒè®Šæ•¸è®€å– API key
- æ¸¬è©¦æ˜¯å¦èƒ½æ­£ç¢ºé€£ç·š
- å›å‚³ä¸€å€‹ OpenAI client

ç¯„ä¾‹ï¼š
```python
from scraper import create_secure_openai_client

client = create_secure_openai_client()

if client:
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    print(response.choices[0].message)
```

---

## ğŸ“¦ Data Contractsï¼ˆPydantic Schemasï¼‰

### Article Schema
```python
from pydantic import BaseModel, Field
from typing import List

class WikipediaExtraction(BaseModel):
    title: str = Field(description="Article's Title")
    description: str = Field(description="Article's Summary or Description or Overview")
    advantages: List[str] = Field(description="The advantages of the topic mentioned in the article")
    disadvantages: List[str] = Field(description="Known challenges or limitations")
    related_concepts: List[str] = Field(description="Related technology, see also")
    notable_methods: List[str] = Field(description="Notable methods, models, or techniques in this area")
```
> è¨»ï¼šå¯¦éš›æ¬„ä½å¯ä¾ä½œæ¥­æœ€çµ‚ schemaï¼ˆå¦‚ `summary`, `key_concepts`, `applications`ï¼‰èª¿æ•´ã€‚

---

## ğŸ§  Agent Classï¼ˆç‹€æ…‹èˆ‡è¡Œç‚ºï¼‰

```python
class Agent:
    def __init__(self, model: str = "gpt-4o-mini"):
        self.articles: List[WikipediaExtraction] = []
        self.model = model
        self.create_secure_openai_client()

    def create_secure_openai_client(self):
    """
    Create OpenAI client with secure API key handling.

    This function:
    1. Looks for OPENAI_API_KEY in environment variables
    2. Tests the connection with a simple API call
    3. Returns the client or None if setup fails
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ No OPENAI_API_KEY found in environment variables")
        print("ğŸ’¡ Set environment variable: OPENAI_API_KEY=your_key")
        return None
    try:
        self.client = OpenAI(api_key=api_key)
        # Test connection with a simple API call
        models = client.models.list()
        print("âœ… OpenAI client created and tested successfully")

    except Exception as e:
        print(f"âŒ OpenAI client creation failed: {e}")
        print("ğŸ” Check your API key and internet connection")
        self.client = None

    # ---- Data In/Out ----
    def add_article(self, article: WikipediaExtraction) -> None:
        self.articles.append(article)

    # def list_titles(self) -> List[str]:
    #     return [a.title for a in self.articles]

    # def get_article_by_title(self, title: str) -> WikipediaExtraction:
    #     for a in self.articles:
    #         if a.title == title:
    #             return a
    #     raise ValueError(f"Article not found: {title}")

    # def get_articles_by_category(self, category: str) -> List[WikipediaExtraction]:
    #     # category å¯èƒ½å°æ‡‰ related_conceptsï¼Œéœ€è‡ªå®šç¾©
    #     results = [a for a in self.articles if category in a.related_concepts]
    #     if not results:
    #         raise ValueError(f"No articles found for category: {category}")
    #     return results

    # ---- Extraction ----
    def extract_structured_data(self, content: str, model: str = "gpt-4o-mini") -> WikipediaExtraction:
        """Use OpenAI structured output to extract data"""
        """
        Extract structured data from raw text using OpenAI's Structured Outputs.

        This replaces the old pattern of:
        response.json()  # Hope it works!

        With guaranteed schema compliance.
        """

        if not client:
            print("âš ï¸ Demo mode: Would extract structured data with OpenAI API")
            return create_mock_wiki_extraction()
        
        # Create the schema for OpenAI
        schema = {
            "name": "wiki_extraction",
            "schema": WikipediaExtraction.model_json_schema(),
            "strict": False  # This enforces strict schema compliance
        }

        try:
            # The magic happens here - response_format enforces our schema
            response = client.chat.completions.create(
                model=self.model,  # Only gpt-4o and gpt-4o-mini support structured outputs
                messages=[
                    {
                        "role": "system",
                        "content": "Please carefully read and comprehend the entire article content provided below. Extract all relevant information and structure it exactly according to the provided schema."
                    },
                    {
                        "role": "user",
                        "content": f"Please carefully analyze the article {content} below. Even if some fields within the schema are not explicitly provided by the article, use inference to fill in any missing information where reasonable. Then, extract and organize the data from the article according to the schema, ensuring completeness and accuracy based on both explicit statements and logical inference."
                    }
                ],
                response_format={
                    "type": "json_schema",
                    "json_schema": schema
                }
            )

            # Direct validation - no parsing errors possible!
            wiki_extraction = WikipediaExtraction.model_validate_json(response.choices[0].message.content)

            print("âœ… Structured extraction successful!")
            print(f"ğŸ“š Processed article: {wiki_extraction.title}")
            print(f"ğŸ’° Topic's Description: {wiki_extraction.description}")
            print(f"â­ Topic's Advantages: {wiki_extraction.advantages}")
            print(f"ğŸ·ï¸ Topic's Disadvantages: {wiki_extraction.disadvantages}")

            return wiki_extraction

        except Exception as e:
            print(f"âŒ Extraction failed: {e}")
            return self.create_mock_wiki_extraction(content)
        
    def batch_extract(self, articles: List[Dict]) -> List[WikipediaExtraction]:
        """Process multiple articles"""
        extracted = []
        for article in articles:
            extracted.append(self.extract_structured_data(article['markdown']))
        
        return extracted
        

    def create_mock_wiki_extraction(self, raw_content: str):
        """Create mock data for demonstration when API is not available."""
        return WikipediaExtraction(
            title="Null",
            description="Null",
            advantages=[],
            disadvantages=[],
            related_concepts=[],
            notable_methods=[]
        )
```

### Function Calling Layer (å°å¤–ä»‹é¢)
```python
def ask_ai(self, query: str, assistant: Agent):
    """
    Function Calling å±¤ï¼ŒAI Assistant èˆ‡å¤–éƒ¨å°è©±çš„å”¯ä¸€å…¥å£ã€‚
    - query: ä½¿ç”¨è€…çš„ä»»å‹™ (ex: "compare_technologies: LLM, RNN")
    - assistant: å·²ç¶“æ“æœ‰ structured articles çš„ Agent
    """
    if query.startswith("compare_technologies"):
        # Example: compare_technologies: LLM, RNN
        techs = query.split(":")[1].split(",")
        results = [assistant.get_article_by_title(t.strip()) for t in techs]
        return results

    elif query.startswith("trace_evolution"):
        # Example: trace_evolution: Deep Learning
        topic = query.split(":")[1].strip()
        return assistant.get_articles_by_category(topic)

    else:
        raise ValueError(f"Unknown query: {query}")
```

**Error Handling**ï¼š  
- è‹¥ `articles` ç‚ºç©ºæˆ–æ‰¾ä¸åˆ°æ¨™é¡Œ â†’ `ValueError`ã€‚  
- `category` å¯ç‚º `None`ï¼›ä½¿ç”¨æ™‚éœ€åšç©ºå€¼æª¢æŸ¥ã€‚  
- è¼¸å…¥ JSON â†’ ä»¥ `Article.model_validate()` é©—è­‰ã€‚  

---

## ğŸ”§ Function Calling Toolsï¼ˆå¤–éƒ¨å·¥å…·è¦æ ¼ï¼‰

> Function Calling ç²¾ç¥ï¼š**LLM æ±ºç­–**æ˜¯å¦å‘¼å«å·¥å…·ï¼Œä½†å·¥å…·æœ¬èº«æ˜¯**å›ºå®šã€å¯æ§**çš„é‚è¼¯ï¼›è‹¥ç„¡ API â†’ é€²å…¥ **Mock Mode**ã€‚  
> é€™å…©å€‹å‡½å¼ä½œç‚ºã€Œå¤–éƒ¨å·¥å…·ã€ï¼Œä¸å°è£åœ¨ Class å…§ï¼ˆç”± Class æä¾›è³‡æ–™çµ¦å®ƒå€‘ï¼‰ã€‚

### 1) `compare_technologies`
**Signature**
```python
def compare_technologies(article_a_json: str, article_b_json: str) -> str:
    """
    æ¯”è¼ƒå…©å€‹æŠ€è¡“æ–‡ç« ï¼ˆä»¥ JSON å­—ä¸²è¼¸å…¥ï¼‰ä¸¦å›å‚³ JSON å­—ä¸²çµæœã€‚

    Parameters:
        article_a_json (str): JSON string of Article
        article_b_json (str): JSON string of Article

    Returns:
        str: JSON string of ArticleComparison
    """
```
**è¼¸å…¥/è¼¸å‡ºæ ¼å¼**  
- **è¼¸å…¥**ï¼š`Article` ç‰©ä»¶åºåˆ—åŒ–å¾Œçš„ **JSON å­—ä¸²**ï¼ˆä¾¿æ–¼ LLM æˆ–è·¨æ¨¡çµ„å‚³éï¼‰ã€‚  
- **è¼¸å‡º**ï¼š`ArticleComparison` ç‰©ä»¶åºåˆ—åŒ–å¾Œçš„ **JSON å­—ä¸²**ã€‚  

**è¡Œç‚ºï¼ˆAPI / Mockï¼‰**  
- **API å¯ç”¨**ï¼šå¯å‘¼å« Gemini/OpenAI å°å…©ç¯‡æ–‡ç« åšå°æ¯”æ‘˜è¦èˆ‡æ¬„ä½å¡«å……ã€‚  
- **Mock**ï¼šæ“·å– `title`ã€`content` å‰ N å­—è£½ä½œæ‘˜è¦ï¼›`common_concepts/unique_*` å¯åŸºæ–¼ç°¡å–®é—œéµå­—é›†åˆæ¯”è¼ƒã€‚  

### 2) `trace_evolution`
**Signature**
```python
from typing import List
def trace_evolution(topic: str, articles_json: List[str]) -> str:
    """
    è¿½è¹¤å–®ä¸€ä¸»é¡Œåœ¨å¤šç¯‡æ–‡ç« ä¸­çš„æ¼”é€²ï¼ˆä»¥ JSON å­—ä¸²é™£åˆ—è¼¸å…¥ã€JSON å­—ä¸²è¼¸å‡ºï¼‰ã€‚

    Parameters:
        topic (str): æŠ€è¡“ä¸»é¡Œ
        articles_json (List[str]): list of JSON string of Article

    Returns:
        str: JSON string (e.g., {'timeline': [...], 'key_innovations': [...], 'notes': str})
    """
```
**è¡Œç‚ºï¼ˆAPI / Mockï¼‰**  
- **API å¯ç”¨**ï¼šè«‹ LLM ä¾æ™‚é–“/æ®µè½æ¨æ¸¬æ¼”é€²è„ˆçµ¡èˆ‡é‡Œç¨‹ç¢‘ã€‚  
- **Mock**ï¼šä»¥æ¨™é¡Œæ’åº + å–æ¯ç¯‡ç¬¬ä¸€æ®µä½œç‚ºã€Œé‡Œç¨‹ç¢‘æ‘˜è¦ã€ã€‚  

---

## ğŸ”„ Pipeline å»ºè­°ï¼ˆé¿å… globalï¼Œæ¸…æ¥šè³‡æ–™æµï¼‰

1. **çˆ¬èŸ²** â†’ å–å¾— raw HTML / textã€‚  
2. **æ¸…ç†** â†’ `clean_content`ï¼ˆç§»é™¤é›œè¨Šã€ä¿ç•™æ®µè½ï¼‰ã€‚  
3. **HTML Parser** â†’ æ“·å–æ¨™é¡Œ/æ®µè½ â†’ dictã€‚  
4. **Pydantic** â†’ é©—è­‰ â†’ `Article` ç‰©ä»¶ â†’ å­˜å…¥ `Agent.articles`ã€‚  
5. **Function Calling**ï¼š
   - å¾ Class å–å‡º `Article`ï¼Œè½‰æˆ **JSON å­—ä¸²**ï¼Œä½œç‚ºå·¥å…·çš„ **input arguments**ã€‚  
   - å‘¼å« `compare_technologies` / `trace_evolution`ï¼ˆAPI æˆ– Mockï¼‰ã€‚  
6. **çµæœå‘ˆç¾**ï¼šå°å‡º / å­˜æª” / å¯è¦–åŒ–ã€‚  

---

## âš ï¸ Error Handling ç­–ç•¥

- **æ²’æœ‰ä»»ä½• Article**ï¼šåœ¨å‘¼å«æ¯”è¼ƒ/æ¼”é€²å‰å…ˆæª¢æŸ¥ï¼Œå¦å‰‡ `ValueError("No articles available")`ã€‚  
- **æ‰¾ä¸åˆ°æŒ‡å®šæ¨™é¡Œ**ï¼š`get_article_by_title` æ‹‹å‡º `ValueError`ã€‚  
- **Category ç‚ºç©º**ï¼šå…è¨± `None`ï¼›åœ¨æª¢ç´¢ç‰¹å®šåˆ†é¡æ™‚é¡¯ç¤ºè­¦å‘Šæˆ–è·³éã€‚  
- **JSON åºåˆ—åŒ–/ååºåˆ—åŒ–**ï¼šä½¿ç”¨ `Article.model_validate_json()` èˆ‡ `model_dump_json()`ã€‚  

---

## ğŸ”— JSON æ ¼å¼ç¯„ä¾‹ï¼ˆArticleï¼‰

```json
{
  "title": "Radiosity (Computer Graphics)",
  "content": "Radiosity is a global illumination algorithm ...",
  "category": "Rendering"
}
```

---

## ğŸ“ ä½¿ç”¨å»ºè­°
- å…§éƒ¨é‹ç®—å»ºè­°ä»¥ **dict/Pydantic** é€²è¡Œï¼›èˆ‡ LLM æˆ–å·¥å…·å±¤äº¤äº’æ™‚å†è½‰ **JSON string**ã€‚  
- ä¿æŒå·¥å…·ï¼ˆå‡½å¼ï¼‰è¼¸å…¥/è¼¸å‡ºåš´æ ¼å®šç¾©ï¼Œæ–¹ä¾¿æ¸¬è©¦èˆ‡æ›¿æ›ï¼ˆAPI â†” Mockï¼‰ã€‚  
- é¿å…ä½¿ç”¨ global stateï¼›ç”± Class çµ±ä¸€ç®¡ç†ç‹€æ…‹èˆ‡è³‡æ–™ã€‚

ğŸ”® æœªä¾†æ“´å……å»ºè­°
- è³‡æ–™å„²å­˜ï¼šå°‡çµæœå­˜å…¥ SQLite / MongoDB / CSV
- è‡ªå‹• Chunkingï¼šä½¿ç”¨ RegexChunking / SlidingWindowChunking
- è³‡è¨Šæ“·å–ï¼šæ­é… LLMExtractionStrategy è‡ªå‹•ç”Ÿæˆçµæ§‹åŒ–çŸ¥è­˜
- å‰ç«¯å±•ç¤ºï¼šæ•´åˆæˆä¸€å€‹å°å‹ Web NotebookLM Demo
